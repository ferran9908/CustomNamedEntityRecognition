{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpacyNoteBook.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ferran9908/CustomNamedEntityRecognition/blob/master/SpacyNoteBook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzSOmj7IHoy3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "from spacy.matcher import PhraseMatcher\n",
        "import plac\n",
        "import random\n",
        "from pathlib import Path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ7p9LXJSC0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Phrase's Indexes\n",
        "# Utility Function\n",
        "def offsetter(lbl,doc,matchedItem):\n",
        "    o_one = len(str(doc[0:matchedItem[1]]))\n",
        "    subDoc = doc[matchedItem[1]:matchedItem[2]]\n",
        "    o_two = o_one + len(str(subDoc))\n",
        "    return (o_one+1,o_two+1,label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8-mOaRySK8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en')\n",
        "\n",
        "if 'ner' not in nlp.pipe_names:\n",
        "    ner = nlp.create_pipe('ner')\n",
        "    nlp.add_pipe(ner)\n",
        "else:\n",
        "    ner = nlp.get_pipe('ner')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_NbCee1SLts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert Text File contents to list\n",
        "diseaseList = []\n",
        "with open('diseaseList.txt','r') as fileObj:\n",
        "    for line in fileObj:\n",
        "        diseaseList.append(line[:-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qDrs9tZSPye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label = 'DISEASE'\n",
        "matcher = PhraseMatcher(nlp.vocab)\n",
        "\n",
        "for i in diseaseList:\n",
        "    matcher.add(label,None,nlp(i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEnM9_pJSRkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test Cell\n",
        "one = nlp(\"Someone has the Abdominal aortic aneurysm and Thrush in men\")\n",
        "matches = matcher(one)\n",
        "\n",
        "matches\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAJFLXDjST4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gather Training Data\n",
        "res = []\n",
        "to_train_ents = []\n",
        "with open('diseases.txt') as fileObj:\n",
        "    line = True\n",
        "    while line:\n",
        "        line = fileObj.readline()\n",
        "        line = line[:-1]\n",
        "        mnlp_line = nlp(line)\n",
        "        matches = matcher(mnlp_line)\n",
        "        res = [offsetter(label,mnlp_line,x) for x in matches]\n",
        "        to_train_ents.append((line,dict(entities=res)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKwsrrpJSlvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del to_train_ents[1890]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR6wpb8BfF1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf8\n",
        "\"\"\"Example of training spaCy's named entity recognizer, starting off with an\n",
        "existing model or a blank model.\n",
        "For more details, see the documentation:\n",
        "* Training: https://spacy.io/usage/training\n",
        "* NER: https://spacy.io/usage/linguistic-features#named-entities\n",
        "Compatible with: spaCy v2.0.0+\n",
        "Last tested with: v2.1.0\n",
        "\"\"\"\n",
        "from __future__ import unicode_literals, print_function\n",
        "\n",
        "import plac\n",
        "import random\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "\n",
        "# training data\n",
        "# TRAIN_DATA = [\n",
        "#     (\"The patient is suffering from Anaemia\", {\"entities\": [(30, 37, \"DISEASE\")]}),\n",
        "#     (\"The patient has Pneumonia and Fever\", {\"entities\": [(16, 25, \"DISEASE\"), (31, 36, \"DISEASE\")]}),\n",
        "#     ('the patient is suffering from Abdominal aortic aneurysm', {'entities': [(30, 55, 'DISEASE')]})\n",
        "# ]\n",
        "\n",
        "\n",
        "@plac.annotations(\n",
        "    model=(\"Model name. Defaults to blank 'en' model.\", \"option\", \"m\", str),\n",
        "    output_dir=(\"Optional output directory\", \"option\", \"o\", Path),\n",
        "    n_iter=(\"Number of training iterations\", \"option\", \"n\", int),\n",
        ")\n",
        "def main(model=None, output_dir=None, n_iter=50):\n",
        "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
        "    if model is not None:\n",
        "        nlp = spacy.load(model)  # load existing spaCy model\n",
        "        print(\"Loaded model '%s'\" % model)\n",
        "    else:\n",
        "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
        "        print(\"Created blank 'en' model\")\n",
        "\n",
        "    # create the built-in pipeline components and add them to the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if \"ner\" not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe(\"ner\")\n",
        "        nlp.add_pipe(ner, last=True)\n",
        "    # otherwise, get it so we can add labels\n",
        "    else:\n",
        "        ner = nlp.get_pipe(\"ner\")\n",
        "\n",
        "    # add labels\n",
        "    for _, annotations in to_train_ents:\n",
        "        for ent in annotations.get(\"entities\"):\n",
        "            ner.add_label(ent[2])\n",
        "\n",
        "    # get names of other pipes to disable them during training\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "        # reset and initialize the weights randomly â€“ but only if we're\n",
        "        # training a new model\n",
        "        if model is None:\n",
        "            nlp.begin_training()\n",
        "        for itn in range(n_iter):\n",
        "            random.shuffle(to_train_ents)\n",
        "            losses = {}\n",
        "            # batch up the examples using spaCy's minibatch\n",
        "            batches = minibatch(to_train_ents, size=compounding(4.0, 32.0, 1.001))\n",
        "            for batch in batches:\n",
        "                texts, annotations = zip(*batch)\n",
        "                nlp.update(\n",
        "                    texts,  # batch of texts\n",
        "                    annotations,  # batch of annotations\n",
        "                    drop=0.5,  # dropout - make it harder to memorise data\n",
        "                    losses=losses,\n",
        "                )\n",
        "            print(\"Iteration:\",itn+1)\n",
        "            print(\"Losses\", losses)\n",
        "\n",
        "    # test the trained model\n",
        "    for text, _ in to_train_ents[:20]:\n",
        "        doc = nlp(text)\n",
        "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
        "\n",
        "    # save model to output directory\n",
        "    # if output_dir is not None:\n",
        "    #     output_dir = Path(output_dir)\n",
        "    #     if not output_dir.exists():\n",
        "    #         output_dir.mkdir()\n",
        "    #     nlp.to_disk(output_dir)\n",
        "    #     print(\"Saved model to\", output_dir)\n",
        "\n",
        "    #     # test the saved model\n",
        "    #     print(\"Loading from\", output_dir)\n",
        "    #     nlp2 = spacy.load(output_dir)\n",
        "    #     for text, _ in to_train_ents:\n",
        "    #         doc = nlp2(text)\n",
        "    #         print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "    #         print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
        "\n",
        "    nlp.to_disk('modelTrained')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "    # Expected output:\n",
        "    # Entities [('Shaka Khan', 'PERSON')]\n",
        "    # Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3),\n",
        "    # ('Khan', 'PERSON', 1), ('?', '', 2)]\n",
        "    # Entities [('London', 'LOC'), ('Berlin', 'LOC')]\n",
        "    # Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3),\n",
        "    # ('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v0ydaqN8xXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4BmRntQgLfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp.to_disk('/content/model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiSByO7X7LGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = [\"It looks like the patient has a high fever\",\"this looks like a case of Cellulitis\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPYfDlkK2NBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp2 = spacy.load('/content/modelTrained')\n",
        "for text in train:\n",
        "  doc = nlp2(text)\n",
        "  print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "  print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHrPH9wQ43s9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqbe-HII6wAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip -r trainedModel modelTrained"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeI5ERh1C7RR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvFzMj8H5CTB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}